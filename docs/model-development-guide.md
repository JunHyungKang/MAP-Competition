# ğŸ¤– ëª¨ë¸ ê°œë°œ ê°€ì´ë“œ

ì´ ê°€ì´ë“œëŠ” MAP-Competition í”„ë¡œì íŠ¸ì—ì„œ ìˆ˜í•™ ì˜¤í•´ ì˜ˆì¸¡ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ë¬¸ì„œ ê°œìš”

### ëª©ì  ë° ë²”ìœ„
ì´ ë¬¸ì„œëŠ” MAP-Competition í”„ë¡œì íŠ¸ì—ì„œ ìˆ˜í•™ ì˜¤í•´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” 3ë‹¨ê³„ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ í›ˆë ¨, í‰ê°€, ì œì¶œ íŒŒì¼ ìƒì„±ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.

### ì£¼ìš” ë‚´ìš©
- 3ë‹¨ê³„ ì˜ˆì¸¡ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„
- ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
- ë‹¤ì–‘í•œ ëª¨ë¸ ì ‘ê·¼ë²• (ê¸°ë³¸/ê³ ê¸‰/ì•™ìƒë¸”)
- ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ í‰ê°€
- ì œì¶œ íŒŒì¼ ìƒì„± ë° ìµœì í™”

### ëŒ€ìƒ ë…ì
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œì— ê´€ì‹¬ì´ ìˆëŠ” ê°œë°œì
- ìì—°ì–´ ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ë¶„ë¥˜ì— ê´€ì‹¬ì´ ìˆëŠ” ì—°êµ¬ì
- MAP ëŒ€íšŒ ì°¸ê°€ì ë° ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### í•µì‹¬ ë‹¨ê³„ ìš”ì•½
1. **í™˜ê²½ ì„¤ì • í™•ì¸** (1ë¶„) - [í™˜ê²½ ì„¤ì • ê°€ì´ë“œ](README.md) ì°¸ì¡°
2. **ë°ì´í„° ë¶„ì„ ì™„ë£Œ** (ì‹œê°„ ê°€ë³€) - [EDA ê°€ì´ë“œ](../eda/README.md) ì°¸ì¡°
3. **ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„** (30ë¶„-1ì‹œê°„)
4. **ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€** (1-3ì‹œê°„)
5. **ì œì¶œ íŒŒì¼ ìƒì„±** (30ë¶„)

### ì£¼ìš” ëª…ë ¹ì–´
```bash
# ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨
python train_basic_model.py

# ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨
python train_advanced_model.py

# ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
python train_ensemble_model.py
```

### ì˜ˆìƒ ì†Œìš” ì‹œê°„
- **ê¸°ë³¸ ëª¨ë¸ ê°œë°œ**: 2-3ì‹œê°„
- **ê³ ê¸‰ ëª¨ë¸ ê°œë°œ**: 4-6ì‹œê°„
- **ì•™ìƒë¸” ëª¨ë¸ ê°œë°œ**: 6-8ì‹œê°„

## ğŸ¯ ëª¨ë¸ ëª©í‘œ

## ğŸ¯ ëª¨ë¸ ëª©í‘œ

### 3ë‹¨ê³„ ì˜ˆì¸¡ ëª¨ë¸
1. **ë‹µë³€ ì •í™•ì„± íŒë‹¨**: True/False ì˜ˆì¸¡
2. **ì˜¤ê°œë… í¬í•¨ ì—¬ë¶€**: Correct/Misconception/Neither ì˜ˆì¸¡
3. **êµ¬ì²´ì  ì˜¤ê°œë… ì‹ë³„**: 35ê°œ ì˜¤ê°œë… ìœ í˜• ì¤‘ í•˜ë‚˜ ì˜ˆì¸¡

### í‰ê°€ ì§€í‘œ
- **MAP@3**: Mean Average Precision @ 3
- ê° ìƒ˜í”Œë‹¹ ìµœëŒ€ 3ê°œ ì˜ˆì¸¡ ê°€ëŠ¥
- ì˜ˆì¸¡ í˜•ì‹: `Category:Misconception`

## ğŸ“Š ë°ì´í„° ì´í•´

### ì…ë ¥ ë°ì´í„°
- **QuestionText**: ìˆ˜í•™ ë¬¸ì œ í…ìŠ¤íŠ¸
- **MC_Answer**: í•™ìƒì´ ì„ íƒí•œ ê°ê´€ì‹ ë‹µ
- **StudentExplanation**: í•™ìƒì˜ ì„¤ëª… í…ìŠ¤íŠ¸

### ì¶œë ¥ ë°ì´í„°
- **Category**: 6ê°œ í´ë˜ìŠ¤
  - True_Correct, True_Misconception, True_Neither
  - False_Correct, False_Misconception, False_Neither
- **Misconception**: 35ê°œ ì˜¤ê°œë… ìœ í˜• + NA

### ë°ì´í„° íŠ¹ì§•
- **ì´ ìƒ˜í”Œ**: 36,696ê°œ (í›ˆë ¨)
- **ê³ ìœ  ë¬¸ì œ**: 15ê°œ
- **í…ìŠ¤íŠ¸ ê¸¸ì´**: í‰ê·  70ì (í•™ìƒ ì„¤ëª…)
- **ë¶ˆê· í˜•**: Categoryì™€ Misconception ë¶„í¬ ë¶ˆê· í˜•

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

### 1. ê¸°ë³¸ ì ‘ê·¼ë²•

#### í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# ëª¨ë¸ ì´ˆê¸°í™”
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, 
    num_labels=len(category_labels)
)

# ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±
def prepare_input(question, answer, explanation):
    return f"Question: {question} Answer: {answer} Explanation: {explanation}"
```

#### ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ
```python
class MultiTaskModel(nn.Module):
    def __init__(self, base_model, num_categories, num_misconceptions):
        super().__init__()
        self.base_model = base_model
        self.category_classifier = nn.Linear(768, num_categories)
        self.misconception_classifier = nn.Linear(768, num_misconceptions)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.base_model(input_ids, attention_mask)
        pooled_output = outputs.pooler_output
        
        category_logits = self.category_classifier(pooled_output)
        misconception_logits = self.misconception_classifier(pooled_output)
        
        return category_logits, misconception_logits
```

### 2. ê³ ê¸‰ ì ‘ê·¼ë²•

#### ê³„ì¸µì  ë¶„ë¥˜
```python
class HierarchicalClassifier:
    def __init__(self):
        # 1ë‹¨ê³„: True/False ë¶„ë¥˜
        self.true_false_classifier = self._build_classifier()
        
        # 2ë‹¨ê³„: Correct/Misconception/Neither ë¶„ë¥˜
        self.correct_misconception_classifier = self._build_classifier()
        
        # 3ë‹¨ê³„: êµ¬ì²´ì  ì˜¤ê°œë… ë¶„ë¥˜
        self.misconception_classifier = self._build_classifier()
    
    def predict(self, text):
        # 1ë‹¨ê³„ ì˜ˆì¸¡
        true_false = self.true_false_classifier.predict(text)
        
        # 2ë‹¨ê³„ ì˜ˆì¸¡
        correct_misconception = self.correct_misconception_classifier.predict(text)
        
        # 3ë‹¨ê³„ ì˜ˆì¸¡ (Misconceptionì´ ìˆëŠ” ê²½ìš°ë§Œ)
        if correct_misconception == "Misconception":
            misconception = self.misconception_classifier.predict(text)
        else:
            misconception = "NA"
        
        return f"{true_false}_{correct_misconception}:{misconception}"
```

#### ì•™ìƒë¸” ëª¨ë¸
```python
class EnsembleModel:
    def __init__(self, models):
        self.models = models
    
    def predict(self, text):
        predictions = []
        for model in self.models:
            pred = model.predict(text)
            predictions.append(pred)
        
        # íˆ¬í‘œ ë˜ëŠ” í‰ê· 
        return self._ensemble_vote(predictions)
```

## ğŸ”§ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 1. í…ìŠ¤íŠ¸ ì •ì œ

#### ê¸°ë³¸ ì •ì œ
```python
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def clean_text(text):
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[^\w\s]', '', text)
    
    # ì†Œë¬¸ì ë³€í™˜
    text = text.lower()
    
    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text)
    tokens = [token for token in tokens if token not in stop_words]
    
    return ' '.join(tokens)
```

#### ìˆ˜í•™ í‘œí˜„ì‹ ì²˜ë¦¬
```python
def process_math_expressions(text):
    # LaTeX í‘œí˜„ì‹ ë³´ì¡´
    latex_pattern = r'\\\(.*?\\\)'
    latex_matches = re.findall(latex_pattern, text)
    
    # ìˆ˜í•™ í‘œí˜„ì‹ì„ í† í°ìœ¼ë¡œ ë³€í™˜
    for i, match in enumerate(latex_matches):
        text = text.replace(match, f"MATH_EXPR_{i}")
    
    return text, latex_matches
```

### 2. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§

#### í…ìŠ¤íŠ¸ íŠ¹ì„±
```python
def extract_text_features(text):
    features = {}
    
    # ê¸°ë³¸ í†µê³„
    features['length'] = len(text)
    features['word_count'] = len(text.split())
    features['avg_word_length'] = np.mean([len(word) for word in text.split()])
    
    # ìˆ˜í•™ ê´€ë ¨ íŠ¹ì„±
    features['math_symbols'] = len(re.findall(r'[+\-*/=<>]', text))
    features['numbers'] = len(re.findall(r'\d+', text))
    features['fractions'] = len(re.findall(r'\d+/\d+', text))
    
    return features
```

#### ê°ì • ë° í†¤ ë¶„ì„
```python
from textblob import TextBlob

def extract_sentiment_features(text):
    blob = TextBlob(text)
    
    features = {}
    features['polarity'] = blob.sentiment.polarity
    features['subjectivity'] = blob.sentiment.subjectivity
    
    return features
```

### 3. ë°ì´í„° ì¦ê°•

#### í…ìŠ¤íŠ¸ ì¦ê°•
```python
from nlpaug.augmenter.word import SynonymAug
from nlpaug.augmenter.sentence import BackTranslationAug

def augment_text(text):
    # ë™ì˜ì–´ ì¹˜í™˜
    synonym_aug = SynonymAug()
    augmented_texts = synonym_aug.augment(text, n=2)
    
    # ì—­ë²ˆì—­
    back_translation_aug = BackTranslationAug()
    back_translated = back_translation_aug.augment(text, n=1)
    
    return augmented_texts + back_translated
```

## ğŸ§  ëª¨ë¸ êµ¬í˜„

### 1. BERT ê¸°ë°˜ ëª¨ë¸

#### ê¸°ë³¸ BERT ë¶„ë¥˜ê¸°
```python
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset

class MathMisconceptionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class BERTClassifier:
    def __init__(self, model_name='bert-base-uncased', num_labels=6):
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=num_labels
        )
    
    def train(self, train_dataloader, val_dataloader, epochs=3):
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)
        
        for epoch in range(epochs):
            self.model.train()
            for batch in train_dataloader:
                optimizer.zero_grad()
                outputs = self.model(**batch)
                loss = outputs.loss
                loss.backward()
                optimizer.step()
    
    def predict(self, text):
        self.model.eval()
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding=True,
            return_tensors='pt'
        )
        
        with torch.no_grad():
            outputs = self.model(**encoding)
            predictions = torch.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(predictions, dim=-1)
        
        return predicted_class.item()
```

### 2. RoBERTa ê¸°ë°˜ ëª¨ë¸

#### RoBERTa ë¶„ë¥˜ê¸°
```python
from transformers import RobertaTokenizer, RobertaForSequenceClassification

class RoBERTaClassifier:
    def __init__(self, model_name='roberta-base', num_labels=6):
        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)
        self.model = RobertaForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels
        )
    
    def prepare_input(self, question, answer, explanation):
        # RoBERTa íŠ¹í™” ì…ë ¥ í˜•ì‹
        return f"Question: {question} <s> Answer: {answer} <s> Explanation: {explanation}"
```

### 3. DeBERTa ê¸°ë°˜ ëª¨ë¸

#### DeBERTa ë¶„ë¥˜ê¸°
```python
from transformers import DebertaTokenizer, DebertaForSequenceClassification

class DeBERTaClassifier:
    def __init__(self, model_name='microsoft/deberta-base', num_labels=6):
        self.tokenizer = DebertaTokenizer.from_pretrained(model_name)
        self.model = DebertaForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels
        )
```

## ğŸ“ˆ í•™ìŠµ ì „ëµ

### 1. ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬

#### ê°€ì¤‘ ì†ì‹¤ í•¨ìˆ˜
```python
import torch.nn.functional as F

def weighted_loss(logits, labels, weights):
    ce_loss = F.cross_entropy(logits, labels, reduction='none')
    weighted_ce_loss = ce_loss * weights[labels]
    return weighted_ce_loss.mean()

# í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
def calculate_class_weights(labels):
    class_counts = torch.bincount(labels)
    total_samples = len(labels)
    class_weights = total_samples / (len(class_counts) * class_counts)
    return class_weights
```

#### ì˜¤ë²„ìƒ˜í”Œë§/ì–¸ë”ìƒ˜í”Œë§
```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

def balance_dataset(X, y):
    # SMOTE ì˜¤ë²„ìƒ˜í”Œë§
    smote = SMOTE(random_state=42)
    X_balanced, y_balanced = smote.fit_resample(X, y)
    
    return X_balanced, y_balanced
```

### 2. êµì°¨ ê²€ì¦

#### Stratified K-Fold
```python
from sklearn.model_selection import StratifiedKFold

def cross_validate_model(X, y, model_class, n_splits=5):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    scores = []
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        model = model_class()
        model.train(X_train, y_train)
        score = model.evaluate(X_val, y_val)
        scores.append(score)
        
        print(f"Fold {fold + 1}: {score:.4f}")
    
    return np.mean(scores), np.std(scores)
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

#### Optunaë¥¼ ì‚¬ìš©í•œ ìµœì í™”
```python
import optuna

def objective(trial):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    max_length = trial.suggest_categorical('max_length', [256, 512, 768])
    
    # ëª¨ë¸ í•™ìŠµ
    model = BERTClassifier()
    score = train_and_evaluate(model, lr, batch_size, max_length)
    
    return score

# ìµœì í™” ì‹¤í–‰
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

## ğŸ¯ ì˜ˆì¸¡ ë° í›„ì²˜ë¦¬

### 1. Top-3 ì˜ˆì¸¡ ìƒì„±

#### í™•ë¥  ê¸°ë°˜ ì˜ˆì¸¡
```python
def generate_top3_predictions(model, text):
    # ëª¨ë¸ ì˜ˆì¸¡
    logits = model.predict_proba(text)
    
    # Top-3 ì¸ë±ìŠ¤
    top3_indices = np.argsort(logits)[-3:][::-1]
    
    # ì˜ˆì¸¡ í˜•ì‹ ë³€í™˜
    predictions = []
    for idx in top3_indices:
        category = category_labels[idx]
        misconception = misconception_labels[idx]
        predictions.append(f"{category}:{misconception}")
    
    return ' '.join(predictions)
```

#### ì•™ìƒë¸” ì˜ˆì¸¡
```python
def ensemble_predict(models, text):
    all_predictions = []
    
    for model in models:
        pred = model.predict_proba(text)
        all_predictions.append(pred)
    
    # í‰ê·  í™•ë¥ 
    avg_probs = np.mean(all_predictions, axis=0)
    
    # Top-3 ì„ íƒ
    top3_indices = np.argsort(avg_probs)[-3:][::-1]
    
    return format_predictions(top3_indices)
```

### 2. í›„ì²˜ë¦¬ ê·œì¹™

#### ë…¼ë¦¬ì  ì œì•½ ì¡°ê±´
```python
def apply_logical_constraints(predictions):
    constrained_predictions = []
    
    for pred in predictions:
        category, misconception = pred.split(':')
        
        # ê·œì¹™ 1: True_Correctì—ëŠ” Misconceptionì´ ì—†ì–´ì•¼ í•¨
        if category == 'True_Correct' and misconception != 'NA':
            misconception = 'NA'
        
        # ê·œì¹™ 2: False_CorrectëŠ” ë…¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥
        if category == 'False_Correct':
            continue
        
        constrained_predictions.append(f"{category}:{misconception}")
    
    return constrained_predictions
```

## ğŸ“Š í‰ê°€ ë° ë¶„ì„

### 1. MAP@3 ê³„ì‚°

#### ì§ì ‘ êµ¬í˜„
```python
def calculate_map_at_3(y_true, y_pred, k=3):
    """
    MAP@3 ê³„ì‚°
    """
    def ap_at_k(y_true, y_pred, k):
        if len(y_pred) == 0:
            return 0.0
        
        # ì •í™•í•œ ì˜ˆì¸¡ì´ ìˆëŠ”ì§€ í™•ì¸
        correct = 0
        ap = 0.0
        
        for i, pred in enumerate(y_pred[:k]):
            if pred == y_true:
                correct += 1
                ap += correct / (i + 1)
        
        return ap / min(k, len(y_pred))
    
    aps = []
    for true, preds in zip(y_true, y_pred):
        ap = ap_at_k(true, preds, k)
        aps.append(ap)
    
    return np.mean(aps)
```

### 2. ì˜¤ë¥˜ ë¶„ì„

#### í˜¼ë™ í–‰ë ¬ ë¶„ì„
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

def analyze_errors(y_true, y_pred, labels):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()
    
    return cm
```

#### í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„
```python
def analyze_text_patterns(df, predictions, errors):
    # ì˜¤ë¥˜ê°€ ë§ì€ í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„
    error_texts = df[errors]['StudentExplanation']
    
    # ê¸¸ì´ ë¶„ì„
    error_lengths = error_texts.str.len()
    print(f"ì˜¤ë¥˜ í…ìŠ¤íŠ¸ í‰ê·  ê¸¸ì´: {error_lengths.mean():.2f}")
    
    # ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ ë¶„ì„
    error_words = ' '.join(error_texts).split()
    word_counts = Counter(error_words)
    print("ì˜¤ë¥˜ í…ìŠ¤íŠ¸ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´:")
    print(word_counts.most_common(10))
```

## ğŸš€ ë°°í¬ ë° ì œì¶œ

### 1. ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ

#### PyTorch ëª¨ë¸
```python
# ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), 'best_model.pth')

# ëª¨ë¸ ë¡œë“œ
model = BERTClassifier()
model.load_state_dict(torch.load('best_model.pth'))
model.eval()
```

#### ONNX ë³€í™˜
```python
import torch.onnx

# ONNX ë³€í™˜
dummy_input = torch.randn(1, 512)
torch.onnx.export(
    model, 
    dummy_input, 
    "model.onnx",
    export_params=True,
    opset_version=11
)
```

### 2. ì œì¶œ íŒŒì¼ ìƒì„±

#### CSV í˜•ì‹
```python
def create_submission_file(test_df, model):
    predictions = []
    
    for _, row in test_df.iterrows():
        text = prepare_input(
            row['QuestionText'],
            row['MC_Answer'],
            row['StudentExplanation']
        )
        
        pred = generate_top3_predictions(model, text)
        predictions.append(pred)
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission_df = pd.DataFrame({
        'row_id': test_df['row_id'],
        'Category:Misconception': predictions
    })
    
    submission_df.to_csv('submission.csv', index=False)
    return submission_df
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```python
import wandb

def log_training_metrics(epoch, train_loss, val_loss, val_map):
    wandb.log({
        'epoch': epoch,
        'train_loss': train_loss,
        'val_loss': val_loss,
        'val_map': val_map
    })
```

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

ëª¨ë¸ ê°œë°œì´ ì™„ë£Œë˜ë©´ ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ê³ ê¸‰ ë„êµ¬ë¥¼ í™œìš©í•˜ì„¸ìš”:

1. **[ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œ](README.md)**: LangChainê³¼ LangGraphë¥¼ í™œìš©í•œ ê³ ê¸‰ ëª¨ë¸ ê°œë°œ ë° ì›Œí¬í”Œë¡œìš° êµ¬ì¶•

### ê¶Œì¥ ì§„í–‰ ìˆœì„œ
1. **ê¸°ë³¸ ëª¨ë¸ ê°œë°œ**: ì´ ê°€ì´ë“œë¥¼ ì°¸ì¡°í•˜ì—¬ ê¸°ë³¸ ëª¨ë¸ êµ¬ì¶•
2. **ì„±ëŠ¥ ìµœì í™”**: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•
3. **ê³ ê¸‰ ë„êµ¬ í™œìš©**: [ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œ](README.md)ë¥¼ ì°¸ì¡°í•˜ì—¬ LLM ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
4. **ìµœì¢… ì œì¶œ**: ëª¨ë“  ëª¨ë¸ì„ í†µí•©í•˜ì—¬ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ìœ ìš©í•œ ë§í¬
- [Transformers ê³µì‹ ë¬¸ì„œ](https://huggingface.co/docs/transformers/)
- [PyTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/)
- [Scikit-learn ê³µì‹ ë¬¸ì„œ](https://scikit-learn.org/)
- [Optuna ê³µì‹ ë¬¸ì„œ](https://optuna.org/)

### ë…¼ë¬¸ ë° ì°¸ê³  ìë£Œ
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ëª¨ë¸ ê°œë°œì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

- [ ] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ
- [ ] ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„
- [ ] í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] êµì°¨ ê²€ì¦ ìˆ˜í–‰
- [ ] ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•
- [ ] ì˜ˆì¸¡ í›„ì²˜ë¦¬ êµ¬í˜„
- [ ] ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ
- [ ] ì œì¶œ íŒŒì¼ ìƒì„±
- [ ] ì½”ë“œ ì •ë¦¬ ë° ë¬¸ì„œí™”

ëª¨ë“  í•­ëª©ì´ ì²´í¬ë˜ë©´ ëª¨ë¸ ê°œë°œì´ ì™„ë£Œëœ ê²ƒì…ë‹ˆë‹¤! ğŸ‰ 
# LangChain & LangGraph ê°€ì´ë“œ - MAP ëŒ€íšŒ

## ğŸ¯ ì„¤ì¹˜ëœ LangChain & LangGraph íŒ¨í‚¤ì§€ë“¤

MAP ëŒ€íšŒë¥¼ ìœ„í•œ LangChainê³¼ LangGraph ê´€ë ¨ íŒ¨í‚¤ì§€ë“¤ì´ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤.

### ğŸ“¦ ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ëª©ë¡

| íŒ¨í‚¤ì§€ëª… | ë²„ì „ | ì„¤ëª… |
|----------|------|------|
| `langchain` | 0.3.26 | LangChain í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ |
| `langchain-community` | 0.3.27 | ì»¤ë®¤ë‹ˆí‹° í†µí•© |
| `langchain-core` | 0.3.69 | í•µì‹¬ ê¸°ëŠ¥ |
| `langchain-openai` | 0.3.28 | OpenAI í†µí•© |
| `langchain-anthropic` | 0.3.17 | Anthropic í†µí•© |
| `langchain-google-genai` | 2.1.8 | Google Gemini í†µí•© |
| `langgraph` | 0.5.3 | LangGraph (ì›Œí¬í”Œë¡œìš°) |
| `langchain-experimental` | 0.3.4 | ì‹¤í—˜ì  ê¸°ëŠ¥ |
| `langsmith` | 0.4.8 | LangSmith (ëª¨ë‹ˆí„°ë§) |

## ğŸš€ MAP ëŒ€íšŒì—ì„œì˜ í™œìš© ë°©ì•ˆ

### 1. **LangChainì„ í™œìš©í•œ í”„ë¡¬í”„íŠ¸ ì²´ì¸**

```python
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_community.llms import Ollama

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
prompt = PromptTemplate.from_template(
    "ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œì™€ í•™ìƒì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ì˜¤ê°œë…ì„ ì‹ë³„í•˜ì„¸ìš”:\n\n"
    "ë¬¸ì œ: {question}\n"
    "í•™ìƒ ë‹µë³€: {student_answer}\n"
    "í•™ìƒ ì„¤ëª…: {student_explanation}\n\n"
    "ë¶„ì„:"
)

# ì˜¤í”ˆì†ŒìŠ¤ LLM ì‚¬ìš© (Ollama)
llm = Ollama(model="phi2")

# ì²´ì¸ ìƒì„±
chain = prompt | llm | StrOutputParser()

# ì‚¬ìš© ì˜ˆì‹œ
result = chain.invoke({
    "question": "0.355ì™€ 0.8 ì¤‘ ì–´ëŠ ê²ƒì´ ë” í°ê°€ìš”?",
    "student_answer": "0.355",
    "student_explanation": "355ê°€ 8ë³´ë‹¤ í¬ë‹ˆê¹Œ 0.355ê°€ ë” ì»¤ìš”"
})
```

### 2. **LangGraphë¥¼ í™œìš©í•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°**

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
from langchain_core.messages import HumanMessage

# ìƒíƒœ ì •ì˜
class AnalysisState(TypedDict):
    question: str
    student_answer: str
    student_explanation: str
    correctness_analysis: str
    misconception_analysis: str
    final_prediction: str

# ë…¸ë“œ í•¨ìˆ˜ë“¤
def analyze_correctness(state: AnalysisState) -> AnalysisState:
    """ë‹µì˜ ì •í™•ì„± ë¶„ì„"""
    # êµ¬í˜„...
    return state

def analyze_misconception(state: AnalysisState) -> AnalysisState:
    """ì˜¤ê°œë… ë¶„ì„"""
    # êµ¬í˜„...
    return state

def generate_prediction(state: AnalysisState) -> AnalysisState:
    """ìµœì¢… ì˜ˆì¸¡ ìƒì„±"""
    # êµ¬í˜„...
    return state

# ê·¸ë˜í”„ ìƒì„±
workflow = StateGraph(AnalysisState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("correctness", analyze_correctness)
workflow.add_node("misconception", analyze_misconception)
workflow.add_node("prediction", generate_prediction)

# ì—£ì§€ ì¶”ê°€
workflow.add_edge("correctness", "misconception")
workflow.add_edge("misconception", "prediction")
workflow.add_edge("prediction", END)

# ì»´íŒŒì¼
app = workflow.compile()
```

### 3. **ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”**

```python
from langchain_community.llms import Ollama
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import JsonOutputParser

# ì—¬ëŸ¬ ëª¨ë¸ ì´ˆê¸°í™”
models = {
    "phi2": Ollama(model="phi2"),
    "gemma": Ollama(model="gemma2"),
    "llama": Ollama(model="llama2")
}

# ì•™ìƒë¸” ì²´ì¸
def ensemble_analysis(input_data):
    results = {}
    for name, model in models.items():
        chain = prompt | model | JsonOutputParser()
        results[name] = chain.invoke(input_data)
    return results

# ì‚¬ìš©
ensemble_result = ensemble_analysis({
    "question": "ìˆ˜í•™ ë¬¸ì œ",
    "student_answer": "í•™ìƒ ë‹µë³€",
    "student_explanation": "í•™ìƒ ì„¤ëª…"
})
```

## ğŸ’¡ MAP ëŒ€íšŒ íŠ¹í™” í™œìš©ë²•

### 1. **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ê³¼ LangChain ê²°í•©**

```python
from reference_notebooks.extracted_modules_for_opensource_llm import MAPDataProcessor, PromptEngineer

# ë°ì´í„° ì²˜ë¦¬
processor = MAPDataProcessor()
train_data = processor.load_and_preprocess_data("train.csv")
train_data = processor.engineer_correctness_feature(train_data)

# LangChainê³¼ ê²°í•©
from langchain_core.prompts import PromptTemplate

# MAP íŠ¹í™” í”„ë¡¬í”„íŠ¸
map_prompt = PromptTemplate.from_template("""
ìˆ˜í•™ êµìœ¡ ì „ë¬¸ê°€ë¡œì„œ í•™ìƒì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì„¸ìš”.

ë¬¸ì œ: {question}
í•™ìƒ ë‹µë³€: {mc_answer}
ë‹µì˜ ì •í™•ì„±: {is_correct}
í•™ìƒ ì„¤ëª…: {student_explanation}

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:
1. True_Correct: ì •ë‹µì´ê³  ì„¤ëª…ë„ ì •í™•
2. True_Misconception: ì •ë‹µì´ì§€ë§Œ ì„¤ëª…ì— ì˜¤ê°œë…
3. False_Correct: ì˜¤ë‹µì´ì§€ë§Œ ì„¤ëª…ì€ ì •í™•
4. False_Misconception: ì˜¤ë‹µì´ê³  ì„¤ëª…ì—ë„ ì˜¤ê°œë…
5. False_Neither: ì˜¤ë‹µì´ê³  ì„¤ëª…ë„ ë¶€ì ì ˆ

ë¶„ì„ ê²°ê³¼:
""")

# ì²´ì¸ ìƒì„±
chain = map_prompt | llm | StrOutputParser()
```

### 2. **LangGraphë¥¼ í™œìš©í•œ ë‹¨ê³„ë³„ ë¶„ì„**

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict

class MAPAnalysisState(TypedDict):
    question_id: str
    question_text: str
    mc_answer: str
    student_explanation: str
    is_correct: bool
    step1_analysis: str
    step2_analysis: str
    step3_analysis: str
    final_category: str
    final_misconception: str

def step1_analyze_correctness(state: MAPAnalysisState) -> MAPAnalysisState:
    """1ë‹¨ê³„: ë‹µì˜ ì •í™•ì„± ë¶„ì„"""
    # êµ¬í˜„...
    return state

def step2_analyze_explanation(state: MAPAnalysisState) -> MAPAnalysisState:
    """2ë‹¨ê³„: ì„¤ëª… ë¶„ì„"""
    # êµ¬í˜„...
    return state

def step3_identify_misconception(state: MAPAnalysisState) -> MAPAnalysisState:
    """3ë‹¨ê³„: ì˜¤ê°œë… ì‹ë³„"""
    # êµ¬í˜„...
    return state

# ì›Œí¬í”Œë¡œìš° ìƒì„±
workflow = StateGraph(MAPAnalysisState)
workflow.add_node("step1", step1_analyze_correctness)
workflow.add_node("step2", step2_analyze_explanation)
workflow.add_node("step3", step3_identify_misconception)

workflow.add_edge("step1", "step2")
workflow.add_edge("step2", "step3")
workflow.add_edge("step3", END)

app = workflow.compile()
```

### 3. **LangSmithë¥¼ í™œìš©í•œ ì‹¤í—˜ ì¶”ì **

```python
import os
from langsmith import Client

# LangSmith ì„¤ì •
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "map-competition"

# ì‹¤í—˜ ì¶”ì 
client = Client()

# ì²´ì¸ ì‹¤í–‰ ë° ì¶”ì 
with client.trace("map-analysis") as tracer:
    result = chain.invoke({
        "question": "ìˆ˜í•™ ë¬¸ì œ",
        "mc_answer": "í•™ìƒ ë‹µë³€",
        "is_correct": True,
        "student_explanation": "í•™ìƒ ì„¤ëª…"
    })
```

## ğŸ”§ ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

### 1. **ê¸°ë³¸ ì‚¬ìš©ë²•**

```python
# ê°€ìƒí™˜ê²½ í™œì„±í™” í›„ (.venv)
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate

# ëª¨ë¸ ì´ˆê¸°í™”
llm = Ollama(model="phi2")

# í”„ë¡¬í”„íŠ¸ ìƒì„±
prompt = PromptTemplate.from_template(
    "ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ ë¶„ì„í•˜ì„¸ìš”: {question}"
)

# ì²´ì¸ ìƒì„±
chain = prompt | llm

# ì‹¤í–‰
result = chain.invoke({"question": "0.355 vs 0.8"})
print(result)
```

### 2. **ê³ ê¸‰ ì‚¬ìš©ë²• (LangGraph)**

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict

class MathAnalysisState(TypedDict):
    input_text: str
    question_analysis: str
    misconception_detection: str
    final_output: str

def analyze_question(state: MathAnalysisState) -> MathAnalysisState:
    # ì§ˆë¬¸ ë¶„ì„ ë¡œì§
    return state

def detect_misconception(state: MathAnalysisState) -> MathAnalysisState:
    # ì˜¤ê°œë… ê°ì§€ ë¡œì§
    return state

def generate_output(state: MathAnalysisState) -> MathAnalysisState:
    # ìµœì¢… ì¶œë ¥ ìƒì„±
    return state

# ê·¸ë˜í”„ ìƒì„±
workflow = StateGraph(MathAnalysisState)
workflow.add_node("analyze", analyze_question)
workflow.add_node("detect", detect_misconception)
workflow.add_node("output", generate_output)

workflow.add_edge("analyze", "detect")
workflow.add_edge("detect", "output")
workflow.add_edge("output", END)

app = workflow.compile()
```

## ğŸš¨ ì£¼ì˜ì‚¬í•­

### 1. **ë©”ëª¨ë¦¬ ê´€ë¦¬**
- LangGraphëŠ” ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
- í° ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ ë°°ì¹˜ ì²˜ë¦¬ ê³ ë ¤

### 2. **ì˜¤í”ˆì†ŒìŠ¤ LLM ì„¤ì •**
- Ollama ì„¤ì¹˜ í•„ìš”: `brew install ollama`
- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: `ollama pull phi2`

### 3. **ìºê¸€ í™˜ê²½ ì œì•½**
- ì¸í„°ë„· ì ‘ê·¼ ì œí•œìœ¼ë¡œ ì¸í•´ ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš© ê°€ëŠ¥
- ì‚¬ì „ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ í™œìš©

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

1. **Ollama ì„¤ì¹˜**: `brew install ollama`
2. **ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**: `ollama pull phi2`
3. **ì‹¤í—˜ ì‹œì‘**: Jupyter ë…¸íŠ¸ë¶ì—ì„œ LangChain ì‚¬ìš©
4. **ì›Œí¬í”Œë¡œìš° ì„¤ê³„**: LangGraphë¡œ ë³µì¡í•œ ë¶„ì„ ì²´ì¸ êµ¬ì¶•
5. **ì„±ëŠ¥ ìµœì í™”**: LangSmithë¡œ ì‹¤í—˜ ì¶”ì 

ì´ì œ LangChainê³¼ LangGraphë¥¼ í™œìš©í•˜ì—¬ MAP ëŒ€íšŒì—ì„œ ë”ìš± ì •êµí•œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€ 
# ì˜¤í”ˆì†ŒìŠ¤ LLM ì‚¬ìš© ê°€ì´ë“œ - MAP ëŒ€íšŒ

## ğŸ¯ ì¶”ì¶œëœ ìœ ìš©í•œ ëª¨ë“ˆë“¤

ModernBERT ë° Gemma ë…¸íŠ¸ë¶ì—ì„œ ì˜¤í”ˆì†ŒìŠ¤ LLM(Gemma, Phi ë“±) ì‚¬ìš©ì— ìœ ìš©í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ëª¨ë“ˆë“¤ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.

## ğŸ“¦ ì£¼ìš” ì¶”ì¶œ ëª¨ë“ˆ

### 1. **MAPDataProcessor** - ë°ì´í„° ì²˜ë¦¬
```python
# í•µì‹¬ ê¸°ëŠ¥
- ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
- ë‹µì˜ ì •í™•ì„± íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (ê°€ì¥ ì¤‘ìš”!)
- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì •í™•ì„± íŠ¹ì„± ì ìš©
```

### 2. **PromptEngineer** - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
```python
# 3ê°€ì§€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
- basic: ì›ë³¸ ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ í˜•ì‹
- detailed: ì˜¤í”ˆì†ŒìŠ¤ LLMìš© ìƒì„¸ í”„ë¡¬í”„íŠ¸
- instruction: Gemma/Phiìš© ëª…ë ¹í˜• í”„ë¡¬í”„íŠ¸
```

### 3. **TokenizationManager** - í† í¬ë‚˜ì´ì§• ê´€ë¦¬
```python
# ì£¼ìš” ê¸°ëŠ¥
- í† í° ê¸¸ì´ ë¶„ì„ ë° ì‹œê°í™”
- ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
- í•˜ë“œì›¨ì–´ ìµœì í™” ì„¤ì •
```

### 4. **MAP3Metric** - í‰ê°€ ë©”íŠ¸ë¦­
```python
# MAP@3 ê³„ì‚° ë° ì œì¶œ íŒŒì¼ ìƒì„±
- ì»¤ìŠ¤í…€ MAP@3 ë©”íŠ¸ë¦­
- ì œì¶œ í˜•ì‹ ë³€í™˜
```

### 5. **ModelTrainer** - ëª¨ë¸ í›ˆë ¨
```python
# ì˜¤í”ˆì†ŒìŠ¤ LLM í›ˆë ¨ ìµœì í™”
- í•˜ë“œì›¨ì–´ ìë™ ê°ì§€
- fp16/bf16 ìë™ ì„¤ì •
```

### 6. **GemmaModelManager** - Gemma ëª¨ë¸ ê´€ë¦¬ (NEW!)
```python
# Keras NLP ê¸°ë°˜ Gemma ëª¨ë¸ ê´€ë¦¬
- LoRAë¥¼ í†µí•œ íš¨ìœ¨ì  íŒŒì¸íŠœë‹
- JAX ë°±ì—”ë“œ ìµœì í™”
- êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
```

### 7. **GemmaPromptEngineer** - Gemma ì „ìš© í”„ë¡¬í”„íŠ¸ (NEW!)
```python
# Gemma ëª¨ë¸ìš© íŠ¹í™” í”„ë¡¬í”„íŠ¸
- ìˆ˜í•™ ë¬¸ì œ í…œí”Œë¦¿
- ìˆ«ì ë¹„êµ í…œí”Œë¦¿
- ë¶„ìˆ˜ ë¬¸ì œ í…œí”Œë¦¿
- ë‹¨ê³„ë³„ ì¶”ë¡  í…œí”Œë¦¿
```

### 8. **GemmaDataAnalyzer** - Gemma ë°ì´í„° ë¶„ì„ (NEW!)
```python
# Gemma ëª¨ë¸ìš© ë°ì´í„° ë¶„ì„
- ì˜¤í•´ ë¶„í¬ ë¶„ì„
- ì¹´í…Œê³ ë¦¬ ë¶„ì„
- ìƒ˜í”Œ ì§ˆë¬¸ ì¶”ì¶œ
```

## ğŸš€ ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ì²œ ëª¨ë¸

### 1. **Microsoft Phi-2** (ì¶”ì²œ)
```python
model_name = "microsoft/phi-2"
# ì¥ì : ì‘ì€ í¬ê¸°, ë¹ ë¥¸ ì¶”ë¡ , ìˆ˜í•™ ëŠ¥ë ¥ ìš°ìˆ˜
# í¬ê¸°: 2.7B íŒŒë¼ë¯¸í„°
```

### 2. **Google Gemma-2B**
```python
model_name = "google/gemma-2b"
# ì¥ì : ì•ˆì •ì , ë‹¤êµ­ì–´ ì§€ì›
# í¬ê¸°: 2B íŒŒë¼ë¯¸í„°
```

### 3. **Microsoft Phi-1.5**
```python
model_name = "microsoft/phi-1.5"
# ì¥ì : ë¹ ë¥¸ í›ˆë ¨, íš¨ìœ¨ì 
# í¬ê¸°: 1.3B íŒŒë¼ë¯¸í„°
```

### 4. **TinyLlama**
```python
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
# ì¥ì : ë§¤ìš° ê°€ë²¼ì›€, ë¹ ë¥¸ ì¶”ë¡ 
# í¬ê¸°: 1.1B íŒŒë¼ë¯¸í„°
```

## ğŸ’¡ í•µì‹¬ í™œìš© ì „ëµ

### 1. **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìš°ì„ ìˆœìœ„**
```python
# ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±: ë‹µì˜ ì •í™•ì„±
processor = MAPDataProcessor()
train_data = processor.engineer_correctness_feature(train_data)
```

### 2. **í”„ë¡¬í”„íŠ¸ ìµœì í™”**
```python
# ì˜¤í”ˆì†ŒìŠ¤ LLMìš© ìƒì„¸ í”„ë¡¬í”„íŠ¸
prompt_engineer = PromptEngineer()
prompts = prompt_engineer.create_prompts(train_data, template='detailed')
```

### 3. **í† í° ê¸¸ì´ ìµœì í™”**
```python
# ì˜¤í”ˆì†ŒìŠ¤ LLMì€ ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì›
tokenizer_manager = TokenizationManager("microsoft/phi-2", max_length=512)
```

### 4. **í•˜ë“œì›¨ì–´ ìµœì í™”**
```python
# ìë™ í•˜ë“œì›¨ì–´ ê°ì§€ ë° ì„¤ì •
trainer = ModelTrainer("microsoft/phi-2", num_classes=65)
trainer.initialize_model()
```

## ğŸ”§ ì‚¬ìš© ì˜ˆì‹œ

### ê¸°ë³¸ ì‚¬ìš©ë²•
```python
from extracted_modules_for_opensource_llm import *

# 1. ë°ì´í„° ì²˜ë¦¬
processor = MAPDataProcessor()
train_data = processor.load_and_preprocess_data("train.csv")
train_data = processor.engineer_correctness_feature(train_data)

# 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
prompt_engineer = PromptEngineer()
prompts = prompt_engineer.create_prompts(train_data, template='detailed')

# 3. í† í¬ë‚˜ì´ì§•
tokenizer_manager = TokenizationManager("microsoft/phi-2", max_length=512)
token_stats = tokenizer_manager.analyze_token_lengths(prompts)

# 4. ëª¨ë¸ í›ˆë ¨
trainer = ModelTrainer("microsoft/phi-2", num_classes=processor.n_classes)
trainer.initialize_model()
```

## ğŸ¤– Gemma ëª¨ë¸ ì‚¬ìš©ë²• (NEW!)

### 1. **ê¸°ë³¸ Gemma ì„¤ì •**
```python
from extracted_modules_for_opensource_llm import GemmaModelManager, GemmaPromptEngineer, GemmaDataAnalyzer

# Gemma ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
gemma_manager = GemmaModelManager(model_name="gemma_2b_en", backend="jax")

# ëª¨ë¸ ë¡œë“œ
gemma_manager.load_model(sequence_length=512)

# LoRA í™œì„±í™” (íš¨ìœ¨ì  íŒŒì¸íŠœë‹)
gemma_manager.enable_lora(rank=64)

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
gemma_manager.setup_optimizer(learning_rate=5e-5, weight_decay=0.01)

# ëª¨ë¸ ì»´íŒŒì¼
gemma_manager.compile_model()
```

### 2. **ë°ì´í„° ë¶„ì„**
```python
# ë°ì´í„° ë¶„ì„ê¸° ì´ˆê¸°í™”
analyzer = GemmaDataAnalyzer()
analyzer.load_data("train.csv", "test.csv")

# ì˜¤í•´ ë¶„ì„
misconception_analysis = analyzer.analyze_misconceptions()
print(f"ì´ ì˜¤í•´ ìˆ˜: {misconception_analysis['total_misconceptions']}")

# ì¹´í…Œê³ ë¦¬ ë¶„ì„
category_analysis = analyzer.analyze_categories()

# ìƒ˜í”Œ ì§ˆë¬¸ ì¶”ì¶œ
sample_questions = analyzer.get_sample_questions(n=3)
```

### 3. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**
```python
# Gemma ì „ìš© í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´
prompt_engineer = GemmaPromptEngineer()

# ë‹¤ì–‘í•œ í…œí”Œë¦¿ ì‚¬ìš©
prompts = {
    'math_question': prompt_engineer.create_prompt(
        "What is 2+2?", "4", template_type='math_question'
    ),
    'comparison': prompt_engineer.create_prompt(
        "Which is greater?", "6.2", template_type='comparison'
    ),
    'fraction': prompt_engineer.create_prompt(
        "Fraction problem", "1/3", template_type='fraction'
    ),
    'step_by_step': prompt_engineer.create_prompt(
        "Solve step by step", "answer", template_type='step_by_step'
    )
}
```

### 4. **í›ˆë ¨ ë° ìƒì„±**
```python
# í›ˆë ¨ ë°ì´í„° ìƒì„±
training_data = gemma_manager.create_training_data(test_df)

# ëª¨ë¸ í›ˆë ¨
history = gemma_manager.train(
    training_data=training_data,
    epochs=1,
    batch_size=1
)

# í…ìŠ¤íŠ¸ ìƒì„±
response = gemma_manager.generate_response(
    prompt="Which number is the greatest 6 or 6.2?",
    max_length=256
)
print(response)
```

### 5. **Gemma ëª¨ë¸ì˜ íŠ¹ë³„í•œ ì¥ì **
```python
# 1. êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
# - "Step 1/2", "Therefore" ë“± ë…¼ë¦¬ì  êµ¬ì¡°
# - LaTeX ìˆ˜í•™ í‘œê¸°ë²• ì§€ì›

# 2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
# - LoRAë¥¼ í†µí•œ íš¨ìœ¨ì  íŒŒì¸íŠœë‹
# - JAX ë°±ì—”ë“œ ìµœì í™”

# 3. ìˆ˜í•™ ëŠ¥ë ¥
# - ìˆ˜í•™ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ ìš°ìˆ˜
# - ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì • ì œì‹œ
```

### ê³ ê¸‰ ì‚¬ìš©ë²• (ì•™ìƒë¸”)
```python
# ì—¬ëŸ¬ ì˜¤í”ˆì†ŒìŠ¤ LLM ì•™ìƒë¸”
models = [
    "microsoft/phi-2",
    "google/gemma-2b", 
    "microsoft/phi-1.5"
]

results = []
for model_name in models:
    trainer = ModelTrainer(model_name, num_classes=processor.n_classes)
    # í›ˆë ¨ ë° ì˜ˆì¸¡
    # results.append(predictions)
```

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. **ë©”ëª¨ë¦¬ ìµœì í™”**
```python
# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
training_args = TrainingArguments(
    gradient_checkpointing=True,
    per_device_train_batch_size=8,  # ì‘ì€ ë°°ì¹˜ í¬ê¸°
    gradient_accumulation_steps=4,   # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
)
```

### 2. **ì†ë„ ìµœì í™”**
```python
# í˜¼í•© ì •ë°€ë„ í›ˆë ¨
training_args = TrainingArguments(
    fp16=True,  # T4 GPUìš©
    bf16=False, # T4ëŠ” bf16 ë¯¸ì§€ì›
)
```

### 3. **ì •í™•ë„ ìµœì í™”**
```python
# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
training_args = TrainingArguments(
    learning_rate=3e-5,  # ë” ì‘ì€ í•™ìŠµë¥ 
    warmup_steps=100,    # ì›Œë°ì—…
    weight_decay=0.01,   # ì •ê·œí™”
)
```

## ğŸ¯ ì˜¤í”ˆì†ŒìŠ¤ LLMì˜ ì¥ì 

### 1. **ì¸í„°ë„· ì œí•œ ì—†ìŒ**
- ìºê¸€ ì½”ë“œ ëŒ€íšŒì˜ "ì¸í„°ë„· ì ‘ê·¼ ë¹„í™œì„±í™”" ì œì•½ í•´ê²°
- ì™„ì „ ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ì‘ë™

### 2. **ì»¤ìŠ¤í„°ë§ˆì´ì§• ììœ ë„**
- ëª¨ë¸ ì•„í‚¤í…ì²˜ ìˆ˜ì • ê°€ëŠ¥
- íŠ¹ì • ë„ë©”ì¸ì— ë§ëŠ” íŒŒì¸íŠœë‹

### 3. **ë¹„ìš© íš¨ìœ¨ì„±**
- API í˜¸ì¶œ ë¹„ìš© ì—†ìŒ
- ë¬´ì œí•œ ì‚¬ìš© ê°€ëŠ¥

### 4. **íˆ¬ëª…ì„±**
- ëª¨ë¸ ë‚´ë¶€ ë™ì‘ ì™„ì „ ì´í•´
- ë””ë²„ê¹… ë° ê°œì„  ìš©ì´

## ğŸš¨ ì£¼ì˜ì‚¬í•­

### 1. **ëª¨ë¸ í¬ê¸° ì œí•œ**
- ìºê¸€ í™˜ê²½ì˜ ë©”ëª¨ë¦¬ ì œì•½ ê³ ë ¤
- ì‘ì€ ëª¨ë¸ ìš°ì„  ì„ íƒ

### 2. **í›ˆë ¨ ì‹œê°„**
- ì˜¤í”ˆì†ŒìŠ¤ LLMì€ í›ˆë ¨ ì‹œê°„ì´ ê¸¸ ìˆ˜ ìˆìŒ
- 9ì‹œê°„ ì œí•œ ë‚´ì—ì„œ ì™„ë£Œ ê°€ëŠ¥í•œ ì„¤ì • í•„ìš”

### 3. **í•˜ë“œì›¨ì–´ í˜¸í™˜ì„±**
- Kaggle T4 GPU íŠ¹ì„± ê³ ë ¤
- fp16 ì‚¬ìš©, bf16 ë¹„í™œì„±í™”

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥

### ModernBERT vs ì˜¤í”ˆì†ŒìŠ¤ LLM ë¹„êµ
| ëª¨ë¸ | í¬ê¸° | ì˜ˆìƒ CV | ì¥ì  | ë‹¨ì  |
|------|------|---------|------|------|
| ModernBERT-large | 355M | 0.938 | ê²€ì¦ëœ ì„±ëŠ¥ | API ì˜ì¡´ |
| Phi-2 | 2.7B | 0.92-0.95 | ì™„ì „ ì˜¤í”„ë¼ì¸ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ |
| **Gemma-2B** | **2B** | **0.90-0.93** | **êµ¬ì¡°í™”ëœ ë‹µë³€, ìˆ˜í•™ ëŠ¥ë ¥** | **ëŠë¦° ì¶”ë¡ ** |
| Phi-1.5 | 1.3B | 0.88-0.92 | ë¹ ë¦„ | ì„±ëŠ¥ ì œí•œ |

## ğŸ¯ ì¶”ì²œ ì ‘ê·¼ë²•

1. **Phi-2ë¡œ ì‹œì‘** - ê°€ì¥ ê· í˜•ì¡íŒ ì„ íƒ
2. **Gemma-2B ê³ ë ¤** - êµ¬ì¡°í™”ëœ ë‹µë³€ê³¼ ìˆ˜í•™ ëŠ¥ë ¥ í™œìš©
3. **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìš°ì„ ** - ë‹µì˜ ì •í™•ì„± ì •ë³´ í™œìš©
4. **í”„ë¡¬í”„íŠ¸ ìµœì í™”** - ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
5. **ì•™ìƒë¸” ê³ ë ¤** - ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©

ì´ ì¶”ì¶œëœ ëª¨ë“ˆë“¤ì„ í™œìš©í•˜ë©´ ì˜¤í”ˆì†ŒìŠ¤ LLMìœ¼ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! 
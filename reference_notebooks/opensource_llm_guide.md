# μ¤ν”μ†μ¤ LLM μ‚¬μ© κ°€μ΄λ“ - MAP λ€ν

## π― μ¶”μ¶λ μ μ©ν• λ¨λ“λ“¤

ModernBERT λ…ΈνΈλ¶μ—μ„ μ¤ν”μ†μ¤ LLM(Gemma, Phi λ“±) μ‚¬μ©μ— μ μ©ν•κ² ν™μ©ν•  μ μλ” ν•µμ‹¬ λ¨λ“λ“¤μ„ μ¶”μ¶ν–μµλ‹λ‹¤.

## π“¦ μ£Όμ” μ¶”μ¶ λ¨λ“

### 1. **MAPDataProcessor** - λ°μ΄ν„° μ²λ¦¬
```python
# ν•µμ‹¬ κΈ°λ¥
- λ°μ΄ν„° λ΅λ“ λ° μ „μ²λ¦¬
- λ‹µμ μ •ν™•μ„± νΉμ„± μ—”μ§€λ‹μ–΄λ§ (κ°€μ¥ μ¤‘μ”!)
- ν…μ¤νΈ λ°μ΄ν„°μ— μ •ν™•μ„± νΉμ„± μ μ©
```

### 2. **PromptEngineer** - ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§
```python
# 3κ°€μ§€ ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ
- basic: μ›λ³Έ λ…ΈνΈλ¶κ³Ό λ™μΌν• ν•μ‹
- detailed: μ¤ν”μ†μ¤ LLMμ© μƒμ„Έ ν”„λ΅¬ν”„νΈ
- instruction: Gemma/Phiμ© λ…λ Ήν• ν”„λ΅¬ν”„νΈ
```

### 3. **TokenizationManager** - ν† ν¬λ‚μ΄μ§• κ΄€λ¦¬
```python
# μ£Όμ” κΈ°λ¥
- ν† ν° κΈΈμ΄ λ¶„μ„ λ° μ‹κ°ν™”
- λ°°μΉ ν† ν¬λ‚μ΄μ§•
- ν•λ“μ›¨μ–΄ μµμ ν™” μ„¤μ •
```

### 4. **MAP3Metric** - ν‰κ°€ λ©”νΈλ¦­
```python
# MAP@3 κ³„μ‚° λ° μ μ¶ νμΌ μƒμ„±
- μ»¤μ¤ν…€ MAP@3 λ©”νΈλ¦­
- μ μ¶ ν•μ‹ λ³€ν™
```

### 5. **ModelTrainer** - λ¨λΈ ν›λ ¨
```python
# μ¤ν”μ†μ¤ LLM ν›λ ¨ μµμ ν™”
- ν•λ“μ›¨μ–΄ μλ™ κ°μ§€
- fp16/bf16 μλ™ μ„¤μ •
```

## π€ μ¤ν”μ†μ¤ LLM μ¶”μ² λ¨λΈ

### 1. **Microsoft Phi-2** (μ¶”μ²)
```python
model_name = "microsoft/phi-2"
# μ¥μ : μ‘μ€ ν¬κΈ°, λΉ λ¥Έ μ¶”λ΅ , μν•™ λ¥λ ¥ μ°μ
# ν¬κΈ°: 2.7B νλΌλ―Έν„°
```

### 2. **Google Gemma-2B**
```python
model_name = "google/gemma-2b"
# μ¥μ : μ•μ •μ , λ‹¤κµ­μ–΄ μ§€μ›
# ν¬κΈ°: 2B νλΌλ―Έν„°
```

### 3. **Microsoft Phi-1.5**
```python
model_name = "microsoft/phi-1.5"
# μ¥μ : λΉ λ¥Έ ν›λ ¨, ν¨μ¨μ 
# ν¬κΈ°: 1.3B νλΌλ―Έν„°
```

### 4. **TinyLlama**
```python
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
# μ¥μ : λ§¤μ° κ°€λ²Όμ›€, λΉ λ¥Έ μ¶”λ΅ 
# ν¬κΈ°: 1.1B νλΌλ―Έν„°
```

## π’΅ ν•µμ‹¬ ν™μ© μ „λµ

### 1. **νΉμ„± μ—”μ§€λ‹μ–΄λ§ μ°μ„ μμ„**
```python
# κ°€μ¥ μ¤‘μ”ν• νΉμ„±: λ‹µμ μ •ν™•μ„±
processor = MAPDataProcessor()
train_data = processor.engineer_correctness_feature(train_data)
```

### 2. **ν”„λ΅¬ν”„νΈ μµμ ν™”**
```python
# μ¤ν”μ†μ¤ LLMμ© μƒμ„Έ ν”„λ΅¬ν”„νΈ
prompt_engineer = PromptEngineer()
prompts = prompt_engineer.create_prompts(train_data, template='detailed')
```

### 3. **ν† ν° κΈΈμ΄ μµμ ν™”**
```python
# μ¤ν”μ†μ¤ LLMμ€ λ” κΈ΄ μ»¨ν…μ¤νΈ μ§€μ›
tokenizer_manager = TokenizationManager("microsoft/phi-2", max_length=512)
```

### 4. **ν•λ“μ›¨μ–΄ μµμ ν™”**
```python
# μλ™ ν•λ“μ›¨μ–΄ κ°μ§€ λ° μ„¤μ •
trainer = ModelTrainer("microsoft/phi-2", num_classes=65)
trainer.initialize_model()
```

## π”§ μ‚¬μ© μμ‹

### κΈ°λ³Έ μ‚¬μ©λ²•
```python
from extracted_modules_for_opensource_llm import *

# 1. λ°μ΄ν„° μ²λ¦¬
processor = MAPDataProcessor()
train_data = processor.load_and_preprocess_data("train.csv")
train_data = processor.engineer_correctness_feature(train_data)

# 2. ν”„λ΅¬ν”„νΈ μƒμ„±
prompt_engineer = PromptEngineer()
prompts = prompt_engineer.create_prompts(train_data, template='detailed')

# 3. ν† ν¬λ‚μ΄μ§•
tokenizer_manager = TokenizationManager("microsoft/phi-2", max_length=512)
token_stats = tokenizer_manager.analyze_token_lengths(prompts)

# 4. λ¨λΈ ν›λ ¨
trainer = ModelTrainer("microsoft/phi-2", num_classes=processor.n_classes)
trainer.initialize_model()
```

### κ³ κΈ‰ μ‚¬μ©λ²• (μ•™μƒλΈ”)
```python
# μ—¬λ¬ μ¤ν”μ†μ¤ LLM μ•™μƒλΈ”
models = [
    "microsoft/phi-2",
    "google/gemma-2b", 
    "microsoft/phi-1.5"
]

results = []
for model_name in models:
    trainer = ModelTrainer(model_name, num_classes=processor.n_classes)
    # ν›λ ¨ λ° μμΈ΅
    # results.append(predictions)
```

## β΅ μ„±λ¥ μµμ ν™” ν

### 1. **λ©”λ¨λ¦¬ μµμ ν™”**
```python
# κ·Έλλ””μ–ΈνΈ μ²΄ν¬ν¬μΈν…
training_args = TrainingArguments(
    gradient_checkpointing=True,
    per_device_train_batch_size=8,  # μ‘μ€ λ°°μΉ ν¬κΈ°
    gradient_accumulation_steps=4,   # κ·Έλλ””μ–ΈνΈ λ„μ 
)
```

### 2. **μ†λ„ μµμ ν™”**
```python
# νΌν•© μ •λ°€λ„ ν›λ ¨
training_args = TrainingArguments(
    fp16=True,  # T4 GPUμ©
    bf16=False, # T4λ” bf16 λ―Έμ§€μ›
)
```

### 3. **μ •ν™•λ„ μµμ ν™”**
```python
# ν•™μµλ¥  μ¤μΌ€μ¤„λ§
training_args = TrainingArguments(
    learning_rate=3e-5,  # λ” μ‘μ€ ν•™μµλ¥ 
    warmup_steps=100,    # μ›λ°μ—…
    weight_decay=0.01,   # μ •κ·ν™”
)
```

## π― μ¤ν”μ†μ¤ LLMμ μ¥μ 

### 1. **μΈν„°λ„· μ ν• μ—†μ**
- μΊκΈ€ μ½”λ“ λ€νμ "μΈν„°λ„· μ ‘κ·Ό λΉ„ν™μ„±ν™”" μ μ•½ ν•΄κ²°
- μ™„μ „ μ¤ν”„λΌμΈ ν™κ²½μ—μ„ μ‘λ™

### 2. **μ»¤μ¤ν„°λ§μ΄μ§• μμ λ„**
- λ¨λΈ μ•„ν‚¤ν…μ² μμ • κ°€λ¥
- νΉμ • λ„λ©”μΈμ— λ§λ” νμΈνλ‹

### 3. **λΉ„μ© ν¨μ¨μ„±**
- API νΈμ¶ λΉ„μ© μ—†μ
- λ¬΄μ ν• μ‚¬μ© κ°€λ¥

### 4. **ν¬λ…μ„±**
- λ¨λΈ λ‚΄λ¶€ λ™μ‘ μ™„μ „ μ΄ν•΄
- λ””λ²„κΉ… λ° κ°μ„  μ©μ΄

## π¨ μ£Όμμ‚¬ν•­

### 1. **λ¨λΈ ν¬κΈ° μ ν•**
- μΊκΈ€ ν™κ²½μ λ©”λ¨λ¦¬ μ μ•½ κ³ λ ¤
- μ‘μ€ λ¨λΈ μ°μ„  μ„ νƒ

### 2. **ν›λ ¨ μ‹κ°„**
- μ¤ν”μ†μ¤ LLMμ€ ν›λ ¨ μ‹κ°„μ΄ κΈΈ μ μμ
- 9μ‹κ°„ μ ν• λ‚΄μ—μ„ μ™„λ£ κ°€λ¥ν• μ„¤μ • ν•„μ”

### 3. **ν•λ“μ›¨μ–΄ νΈν™μ„±**
- Kaggle T4 GPU νΉμ„± κ³ λ ¤
- fp16 μ‚¬μ©, bf16 λΉ„ν™μ„±ν™”

## π“ μμƒ μ„±λ¥

### ModernBERT vs μ¤ν”μ†μ¤ LLM λΉ„κµ
| λ¨λΈ | ν¬κΈ° | μμƒ CV | μ¥μ  | λ‹¨μ  |
|------|------|---------|------|------|
| ModernBERT-large | 355M | 0.938 | κ²€μ¦λ μ„±λ¥ | API μμ΅΄ |
| Phi-2 | 2.7B | 0.92-0.95 | μ™„μ „ μ¤ν”„λΌμΈ | λ©”λ¨λ¦¬ μ‚¬μ©λ‰ |
| Gemma-2B | 2B | 0.90-0.93 | μ•μ •μ  | λλ¦° μ¶”λ΅  |
| Phi-1.5 | 1.3B | 0.88-0.92 | λΉ λ¦„ | μ„±λ¥ μ ν• |

## π― μ¶”μ² μ ‘κ·Όλ²•

1. **Phi-2λ΅ μ‹μ‘** - κ°€μ¥ κ· ν•μ΅ν μ„ νƒ
2. **νΉμ„± μ—”μ§€λ‹μ–΄λ§ μ°μ„ ** - λ‹µμ μ •ν™•μ„± μ •λ³΄ ν™μ©
3. **ν”„λ΅¬ν”„νΈ μµμ ν™”** - μƒμ„Έν• ν”„λ΅¬ν”„νΈ μ‚¬μ©
4. **μ•™μƒλΈ” κ³ λ ¤** - μ—¬λ¬ λ¨λΈ μ΅°ν•©

μ΄ μ¶”μ¶λ λ¨λ“λ“¤μ„ ν™μ©ν•λ©΄ μ¤ν”μ†μ¤ LLMμΌλ΅λ„ λ†’μ€ μ„±λ¥μ„ λ‹¬μ„±ν•  μ μμµλ‹λ‹¤! 